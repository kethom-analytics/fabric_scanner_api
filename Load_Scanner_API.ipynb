{"cells":[{"cell_type":"markdown","source":["Install semantic-link package for sempy usage. This step can be skipped, if semantic-link has been installed to the attached environment"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"3402416b-4844-49cf-adcc-16d58e1aa4c0"},{"cell_type":"code","source":["pip install semantic-link"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":true}},"id":"567b560c-5c63-43e0-bede-d35dcbb080ee"},{"cell_type":"markdown","source":["Import of required python packages"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"b3fb6ea3-2048-46f2-877b-f54e1f12817b"},{"cell_type":"code","source":["import pandas as pd\n","import sempy.fabric as fabric\n","import json\n","from pyspark.sql.types import *\n","import datetime\n","import time"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"19430e80-fbf9-49e9-b05e-fdd312990c4e"},{"cell_type":"markdown","source":["Configuration of parameters:\n","- **workspaces_per_request**: This variable determines, how many workspaces will be requested in a single call. Currently a maximum of 100 workspaces can be requested in a single call\n","- **max_parallel_requests**: This variable determines, how many concurrent requests can be done towards the scanner API. Currently there is a maximum of 16 parallel requests\n","- **write_to_files**: If true, the JSON results will be written to the files of the lakehouse"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"e8c58a7a-6c79-4cdf-a2c0-63dd93d02713"},{"cell_type":"code","source":["workspaces_per_request = 100\n","max_parallel_requests = 16\n","write_to_files = True"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"d87163c6-ba54-47a2-8a7e-eb7a8d218c83"},{"cell_type":"code","source":["#Instantiate the client\n","client = fabric.FabricRestClient()\n","current_time = datetime.datetime.now()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"03debbc8-c048-41d9-a3c5-8f23bc65adf7"},{"cell_type":"markdown","source":["Get capacity data"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"9a653dc1-0939-415a-802b-20b4187edfc5"},{"cell_type":"markdown","source":["Group the total number of workspaces into packages of 100 (workspaces_per_request)"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"bcd7a66c-b832-44f5-909b-d43980bfaedb"},{"cell_type":"code","source":["response = client.get(f\"/v1.0/myorg/admin/workspaces/modified?excludePersonalWorkspaces=True&excludeInActiveWorkspaces=True\")\n","modified_workspaces = pd.json_normalize(response.json())\n","modified_workspaces[\"index\"] = pd.to_numeric(modified_workspaces.reset_index()[\"index\"])\n","modified_workspaces[\"run\"] = modified_workspaces[\"index\"] // workspaces_per_request\n","modified_workspaces = modified_workspaces.groupby('run')['id'].apply(list)\n","df_runs = pd.DataFrame(data = modified_workspaces)\n","df_runs[\"status\"] = \"Not Started\""],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"0927fc72-b0b4-49e9-a27d-ece673806343"},{"cell_type":"markdown","source":["Use getInfo API to request generation of meta data for all workspaces, making sure maximal 16 (workspaces_per_request) requests are running in parallel"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"0c501eb5-5570-4201-ad9b-5bc11a5d797c"},{"cell_type":"code","source":["df_runs_current = df_runs[df_runs[\"status\"].isin([\"Not Started\", \"Request sent\", \"Running\"])].head(max_parallel_requests)\n","\n","while df_runs_current.shape[0] > 0:\n","    time.sleep(5)\n","    for i, row in df_runs_current.iterrows():\n","        if row[\"status\"] == \"Not Started\":\n","            payload = {}\n","            payload[\"workspaces\"] = row[\"id\"]\n","            response = client.post(\"/v1.0/myorg/admin/workspaces/getInfo?getArtifactUsers=True\", json = payload)\n","            id = pd.json_normalize(response.json())[\"id\"][0]\n","            df_runs.loc[i, \"status\"] = \"Request sent\"\n","            df_runs.loc[i, \"run_id\"] = id\n","        elif row[\"status\"] in [ \"Request sent\", \"Running\"]:\n","            response = client.get(\"/v1.0/myorg/admin/workspaces/scanStatus/\" + row[\"run_id\"])\n","            stat = pd.json_normalize(response.json())[\"status\"][0]\n","            df_runs.loc[i, \"status\"] = stat\n","    df_runs_current = df_runs[df_runs[\"status\"].isin([\"Not Started\", \"Request sent\", \"Running\"])].head(max_parallel_requests)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"bfe80f66-fbcf-49da-b462-42820df99c29"},{"cell_type":"markdown","source":["Get scanner api results"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"58a9ffc7-577c-4b63-ba7e-ea663c2aafab"},{"cell_type":"code","source":["results = []\n","for i, row in df_runs.iterrows():\n","    if row[\"status\"] == \"Succeeded\":\n","            response = client.get(f\"/v1.0/myorg/admin/workspaces/scanResult/\" + row[\"run_id\"])\n","            print(\"/v1.0/myorg/admin/workspaces/scanResult/\" + row[\"run_id\"])\n","            results.append(response.json())\n","            if write_to_files:\n","                folder_path = mssparkutils.fs.getMountPath('/default') + \"/Files/Metadata_Requests/\" + current_time.strftime(\"%Y-%m-%d\") + \"/\" +  current_time.strftime(\"%H-%M-%S\") + \"/\"\n","                mssparkutils.fs.mkdirs(f\"file://\" +folder_path)\n","                with open(folder_path + row[\"run_id\"] +\".json\", \"w\") as f:\n","                    f.write(json.dumps(response.json()))\n","                    \n","        "],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"7b9dc0de-c631-4846-a7ad-678ea76272e4"},{"cell_type":"markdown","source":["Function to parse result json into needed tables"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"b584db2c-c745-4609-b445-94418648c173"},{"cell_type":"code","source":["def get_details( df, parent_id, col, **kwargs ):\n","    rename_id = kwargs.get('rename_id' , None)\n","    df_res = df[[parent_id, col]].explode(col, ignore_index = True)\n","    df_res = df_res[[parent_id]].join(pd.json_normalize(df_res[col]))\n","    if not(rename_id is None):\n","        df_res = df_res.dropna(subset=['id']).rename(columns = {'id' : rename_id})\n","    return df_res"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"718922c0-1acb-4644-ba56-24f45d608dce"},{"cell_type":"markdown","source":["Parse the information from the result into data frames which can be written to lakehouse in later step. In case some object types are not existing in Fabric tenant, this might fail. In this case, just comment out the lines which fail"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"4cfd52ca-201b-4ed2-a933-e6a94b44e1cb"},{"cell_type":"code","source":["capacities_response = client.get(f\"/v1.0/myorg/capacities\")\n","df_capacities = pd.json_normalize(pd.json_normalize(capacities_response.json()).explode(\"value\")[\"value\"])\n","\n","df_json = pd.json_normalize(pd.json_normalize(results).explode(\"workspaces\")[\"workspaces\"])\n","df_json = df_json.rename(columns = {'id' : 'workspace_id'})\n","\n","df_workspaces = df_json[[\"workspace_id\", \"name\", \"type\", \"state\", \"isOnDedicatedCapacity\", \"capacityId\", \"defaultDatasetStorageFormat\"]]\n","df_workspace_users = get_details(df_json, \"workspace_id\", \"users\")\n","\n","df_reports = get_details(df_json, \"workspace_id\" , \"reports\", rename_id = \"report_id\")\n","df_report_users = get_details( df_reports, \"report_id\", \"users\")\n","\n","df_datasets = get_details(df_json, \"workspace_id\" , \"datasets\",rename_id =  \"dataset_id\")\n","df_dataset_users = get_details( df_datasets, \"dataset_id\", \"users\")\n","\n","df_dashboards =  get_details( df_json, \"workspace_id\", \"dashboards\",rename_id =  \"dashboard_id\")\n","df_dashboard_users = get_details( df_dashboards, \"dashboard_id\", \"users\")\n","\n","df_lakehouses =  get_details( df_json, \"workspace_id\", \"Lakehouse\", rename_id = \"lakehouse_id\")\n","df_lakehouse_users = get_details( df_lakehouses, \"lakehouse_id\", \"users\")\n","\n","df_warehouses =  get_details( df_json, \"workspace_id\", \"warehouses\", rename_id = \"warehouse_id\")\n","df_warehouse_users = get_details( df_warehouses, \"warehouse_id\", \"users\")\n","\n","df_eventstreams =  get_details( df_json, \"workspace_id\", \"Eventstream\" ,rename_id =  \"eventstream_id\")\n","df_eventstream_users = get_details( df_eventstreams, \"eventstream_id\", \"users\")\n","\n","df_datapipelines =  get_details( df_json, \"workspace_id\", \"DataPipeline\",rename_id =  \"datapipeline_id\")\n","df_datapipeline_users = get_details( df_datapipelines, \"datapipeline_id\", \"users\")\n","\n","df_notebooks =  get_details( df_json, \"workspace_id\", \"Notebook\" ,rename_id =  \"notebook_id\")\n","df_notebook_users = get_details( df_notebooks, \"notebook_id\", \"users\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"1ce9c3d6-00e8-49a2-85ff-0964e2d60ab2"},{"cell_type":"markdown","source":["Function to dynamically write different data frames to lakehouse. Data will be overwritten in Lakehouse"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"e4f64793-f03f-4e33-a932-f7bea58c98de"},{"cell_type":"code","source":["def write_data( df, table_name):\n","    df[\"ts_load\"] = current_time\n","    df = spark.createDataFrame(df)\n","    columns_to_drop = []\n","    for it in df.dtypes:\n","        dtype = it[1]\n","        if dtype == \"array<void>\":\n","            columns_to_drop.append(it[0])\n","\n","    df = df.drop(*columns_to_drop )\n","    df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").format(\"delta\").save(\"Tables/\" + table_name)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"4658a633-2e6d-4cf3-834b-9bb8a15ca869"},{"cell_type":"markdown","source":["Apply function to different data frames in order to write them to Delta format"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"a848303a-ccb0-49cf-82f9-d6a070ad88e7"},{"cell_type":"code","source":["write_list = [\n","{\"df\": df_datasets , \"name\" : \"datasets\"},\n","{\"df\": df_workspaces , \"name\" : \"workspaces\"},\n","{\"df\": df_reports , \"name\" : \"reports\"},\n","{\"df\": df_report_users , \"name\" : \"report_users\"},\n","{\"df\": df_workspace_users , \"name\" : \"workspace_users\"},\n","{\"df\": df_dataset_users , \"name\" : \"dataset_users\"},\n","{\"df\": df_capacities , \"name\" : \"capacities\"},\n","{\"df\": df_lakehouses , \"name\" : \"lakehouses\"},\n","{\"df\": df_lakehouse_users , \"name\" : \"lakehouses_users\"},\n","{\"df\": df_dashboards , \"name\" : \"dashboards\"},\n","{\"df\": df_dashboard_users , \"name\" : \"dashboard_users\"},\n","{\"df\": df_warehouses , \"name\" : \"warehouses\"},\n","{\"df\": df_warehouse_users , \"name\" : \"warehouse_users\"},\n","{\"df\": df_eventstreams , \"name\" : \"eventstreams\"},\n","{\"df\": df_eventstream_users, \"name\" : \"eventstream_users\"},\n","{\"df\": df_datapipelines , \"name\" : \"datapipelines\"},\n","{\"df\": df_datapipeline_users , \"name\" : \"datapipeline_users\"},\n","{\"df\": df_notebooks , \"name\" : \"notebooks\"},\n","{\"df\": df_notebook_users , \"name\" : \"notebook_users\"}\n","]\n","\n","for it in write_list:\n","    write_data(it[\"df\"], it[\"name\"])"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"18e21189-c0dd-4272-9344-0b3eaf8d9ed0"},{"cell_type":"code","source":[],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"84f16433-ef63-4748-8ffb-9895d765cd7a"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default"},"trident":{"lakehouse":{"default_lakehouse":"c9db4ddb-41a9-4a98-a7d4-04d64146a8ca","default_lakehouse_name":"LH_METADATA","default_lakehouse_workspace_id":"8a427b4b-80cb-428d-93b6-d850bab13340"},"environment":{}}},"nbformat":4,"nbformat_minor":5}