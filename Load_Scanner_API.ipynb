{"cells":[{"cell_type":"markdown","source":["Install semantic-link package for sempy usage. This step can be skipped, if semantic-link has been installed to the attached environment"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"3402416b-4844-49cf-adcc-16d58e1aa4c0"},{"cell_type":"code","source":["pip install semantic-link"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":true}},"id":"567b560c-5c63-43e0-bede-d35dcbb080ee"},{"cell_type":"markdown","source":["Import of required python packages"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"b3fb6ea3-2048-46f2-877b-f54e1f12817b"},{"cell_type":"code","source":["import pandas as pd\n","import sempy.fabric as fabric\n","import json\n","from pyspark.sql.types import *\n","import datetime\n","import time\n","pd.options.mode.chained_assignment = None #This option is used to suppress a warning "],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"19430e80-fbf9-49e9-b05e-fdd312990c4e"},{"cell_type":"markdown","source":["Configuration of parameters:\n","- **workspaces_per_request**: This variable determines, how many workspaces will be requested in a single call. Currently a maximum of 100 workspaces can be requested in a single call\n","- **max_parallel_requests**: This variable determines, how many concurrent requests can be done towards the scanner API. Currently there is a maximum of 16 parallel requests\n","- **write_to_files**: If true, the JSON results will be written to the files of the lakehouse\n","- **keep_history**: If true, daily data will be added to the Delta tables, if false, data will be overwritten!"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"e8c58a7a-6c79-4cdf-a2c0-63dd93d02713"},{"cell_type":"code","source":["workspaces_per_request = 100\n","max_parallel_requests = 16\n","write_to_files = True\n","keep_history = True"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"d87163c6-ba54-47a2-8a7e-eb7a8d218c83"},{"cell_type":"code","source":["# Instantiate the client\n","client = fabric.FabricRestClient()\n","current_time = datetime.datetime.now()\n","date = current_time.date()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"03debbc8-c048-41d9-a3c5-8f23bc65adf7"},{"cell_type":"markdown","source":["Group the total number of workspaces into packages of 100 (workspaces_per_request)"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"bcd7a66c-b832-44f5-909b-d43980bfaedb"},{"cell_type":"code","source":["response = client.get(f\"/v1.0/myorg/admin/workspaces/modified?excludePersonalWorkspaces=True&excludeInActiveWorkspaces=True\")\n","modified_workspaces = pd.json_normalize(response.json())\n","modified_workspaces[\"index\"] = pd.to_numeric(modified_workspaces.reset_index()[\"index\"])\n","modified_workspaces[\"run\"] = modified_workspaces[\"index\"] // workspaces_per_request\n","modified_workspaces = modified_workspaces.groupby('run')['id'].apply(list)\n","df_runs = pd.DataFrame(data = modified_workspaces)\n","df_runs[\"status\"] = \"Not Started\""],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"0927fc72-b0b4-49e9-a27d-ece673806343"},{"cell_type":"markdown","source":["Use getInfo API to request generation of meta data for all workspaces, making sure maximal 16 (workspaces_per_request) requests are running in parallel"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"0c501eb5-5570-4201-ad9b-5bc11a5d797c"},{"cell_type":"code","source":["df_runs_current = df_runs[df_runs[\"status\"].isin([\"Not Started\", \"Request sent\", \"Running\"])].head(max_parallel_requests)\n","\n","while df_runs_current.shape[0] > 0:\n","    time.sleep(5)\n","    for i, row in df_runs_current.iterrows():\n","        if row[\"status\"] == \"Not Started\":\n","            payload = {}\n","            payload[\"workspaces\"] = row[\"id\"]\n","            response = client.post(\"/v1.0/myorg/admin/workspaces/getInfo?getArtifactUsers=True\", json = payload)\n","            id = pd.json_normalize(response.json())[\"id\"][0]\n","            df_runs.loc[i, \"status\"] = \"Request sent\"\n","            df_runs.loc[i, \"run_id\"] = id\n","        elif row[\"status\"] in [ \"Request sent\", \"Running\"]:\n","            response = client.get(\"/v1.0/myorg/admin/workspaces/scanStatus/\" + row[\"run_id\"])\n","            stat = pd.json_normalize(response.json())[\"status\"][0]\n","            df_runs.loc[i, \"status\"] = stat\n","    df_runs_current = df_runs[df_runs[\"status\"].isin([\"Not Started\", \"Request sent\", \"Running\"])].head(max_parallel_requests)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"bfe80f66-fbcf-49da-b462-42820df99c29"},{"cell_type":"markdown","source":["Fetch scanner api results. If write_to_files is true the JSON response will also be written into the files section"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"58a9ffc7-577c-4b63-ba7e-ea663c2aafab"},{"cell_type":"code","source":["results = []\n","for i, row in df_runs.iterrows():\n","    if row[\"status\"] == \"Succeeded\":\n","            response = client.get(f\"/v1.0/myorg/admin/workspaces/scanResult/\" + row[\"run_id\"])\n","            print(\"/v1.0/myorg/admin/workspaces/scanResult/\" + row[\"run_id\"])\n","            results.append(response.json())\n","            if write_to_files:\n","                folder_path = mssparkutils.fs.getMountPath('/default') + \"/Files/Metadata_Requests/\" + current_time.strftime(\"%Y-%m-%d\") + \"/\" +  current_time.strftime(\"%H-%M-%S\") + \"/\"\n","                mssparkutils.fs.mkdirs(f\"file://\" +folder_path)\n","                with open(folder_path + row[\"run_id\"] +\".json\", \"w\") as f:\n","                    f.write(json.dumps(response.json()))\n","                    \n","        "],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"7b9dc0de-c631-4846-a7ad-678ea76272e4"},{"cell_type":"markdown","source":["Function to parse result json into needed tables. If object type is not present in tenant, the function will return an empty df instead.\n","The following parameters are expected:\n","- df: Pandas DataFrame to be flattened\n","- parent_id: ID to parent object, to be able to Link the data later on\n","- rename_id (optional): New name of id column"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"b584db2c-c745-4609-b445-94418648c173"},{"cell_type":"code","source":["def get_details( df, parent_id, col, **kwargs ):\n","    try:\n","        rename_id = kwargs.get('rename_id' , None)\n","        df_res = df[[parent_id, col]].explode(col, ignore_index = True)\n","        df_res = df_res[[parent_id]].join(pd.json_normalize(df_res[col]))\n","        if not(rename_id is None):\n","            df_res = df_res.dropna(subset=['id']).rename(columns = {'id' : rename_id})\n","        return df_res\n","    except:\n","        return pd.DataFrame()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"718922c0-1acb-4644-ba56-24f45d608dce"},{"cell_type":"markdown","source":["Parse the information from the result into data frames which can be written to lakehouse in later step. In case some object types are not existing in Fabric tenant, this might fail. In this case, just comment out the lines which fail"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"4cfd52ca-201b-4ed2-a933-e6a94b44e1cb"},{"cell_type":"code","source":["capacities_response = client.get(f\"/v1.0/myorg/capacities\")\n","df_capacities = pd.json_normalize(pd.json_normalize(capacities_response.json()).explode(\"value\")[\"value\"]).rename(columns = {'id' : 'capacity_id'})\n","\n","df_json = pd.json_normalize(pd.json_normalize(results).explode(\"workspaces\")[\"workspaces\"])\n","df_json = df_json.rename(columns = {'id' : 'workspace_id'}).rename(columns = {'capacityId' : 'capacity_id'})\n","\n","df_workspaces = df_json[[\"workspace_id\", \"name\", \"type\", \"state\", \"isOnDedicatedCapacity\", \"capacity_id\", \"defaultDatasetStorageFormat\"]]\n","df_workspace_users = get_details(df_json, \"workspace_id\", \"users\")\n","\n","df_reports = get_details(df_json, \"workspace_id\" , \"reports\", rename_id = \"report_id\")\n","df_report_users = get_details( df_reports, \"report_id\", \"users\")\n","\n","df_datasets = get_details(df_json, \"workspace_id\" , \"datasets\",rename_id =  \"dataset_id\")\n","df_dataset_users = get_details( df_datasets, \"dataset_id\", \"users\")\n","\n","df_dashboards =  get_details( df_json, \"workspace_id\", \"dashboards\",rename_id =  \"dashboard_id\")\n","df_dashboard_users = get_details( df_dashboards, \"dashboard_id\", \"users\")\n","\n","df_lakehouses =  get_details( df_json, \"workspace_id\", \"Lakehouse\", rename_id = \"lakehouse_id\")\n","df_lakehouse_users = get_details( df_lakehouses, \"lakehouse_id\", \"users\")\n","\n","df_warehouses =  get_details( df_json, \"workspace_id\", \"warehouses\", rename_id = \"warehouse_id\")\n","df_warehouse_users = get_details( df_warehouses, \"warehouse_id\", \"users\")\n","\n","df_eventstreams =  get_details( df_json, \"workspace_id\", \"Eventstream\" ,rename_id =  \"eventstream_id\")\n","df_eventstream_users = get_details( df_eventstreams, \"eventstream_id\", \"users\")\n","\n","df_datapipelines =  get_details( df_json, \"workspace_id\", \"DataPipeline\",rename_id =  \"datapipeline_id\")\n","df_datapipeline_users = get_details( df_datapipelines, \"datapipeline_id\", \"users\")\n","\n","df_notebooks =  get_details( df_json, \"workspace_id\", \"Notebook\" ,rename_id =  \"notebook_id\")\n","df_notebook_users = get_details( df_notebooks, \"notebook_id\", \"users\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"1ce9c3d6-00e8-49a2-85ff-0964e2d60ab2"},{"cell_type":"markdown","source":["Function to dynamically write different data frames to lakehouse. Depending on the keep_history variable data get added daily or overwritten in the respective delta tables. If the pandas dataframe is empty, no result will be written to Delta tables\n","The following parameters need to be configured:\n","- df: Pandas Dataframe containing the data to be written\n","- table_name: Target table name\n","- keys: Array of all keys that need to be generated for Power BI to be able to join over two columns"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"e4f64793-f03f-4e33-a932-f7bea58c98de"},{"cell_type":"code","source":["def write_data( df, table_name, keys):\n","    if df.empty:\n","        print(\"No data for table \" + table_name + \" existing\")\n","    else:\n","        df[\"date\"] = date\n","        for k in keys:\n","            df[k.replace(\"_id\", \"_key\")] = str(date) + \"-\" + df[k]\n","        df = spark.createDataFrame(df)\n","        columns_to_drop = []\n","        for it in df.dtypes:\n","            dtype = it[1]\n","            if dtype == \"array<void>\":\n","                columns_to_drop.append(it[0])\n","\n","        df = df.drop(*columns_to_drop )\n","        if keep_history:\n","            if spark.catalog.tableExists(table_name):\n","                spark.sql(\"DELETE FROM \" + table_name + \" WHERE date = '\" + str(date) + \"'\")\n","            df.write.mode(\"append\").option(\"mergeSchema\", \"true\").format(\"delta\").save(\"Tables/\" + table_name)\n","        else:   \n","            df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").format(\"delta\").save(\"Tables/\" + table_name)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"4658a633-2e6d-4cf3-834b-9bb8a15ca869"},{"cell_type":"markdown","source":["Apply function to different data frames in order to write them to Delta format"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"a848303a-ccb0-49cf-82f9-d6a070ad88e7"},{"cell_type":"code","source":["write_list = [\n","{\"df\": df_datasets , \"name\" : \"datasets\", \"keys\" : [\"dataset_id\", \"workspace_id\"]},\n","{\"df\": df_workspaces , \"name\" : \"workspaces\", \"keys\" : [\"workspace_id\", \"capacity_id\"]},\n","{\"df\": df_reports , \"name\" : \"reports\", \"keys\" : [\"report_id\", \"workspace_id\"]},\n","{\"df\": df_report_users , \"name\" : \"report_users\",  \"keys\" : [\"report_id\"]},\n","{\"df\": df_workspace_users , \"name\" : \"workspace_users\", \"keys\" : [\"workspace_id\"]},\n","{\"df\": df_dataset_users , \"name\" : \"dataset_users\", \"keys\" : [\"dataset_id\"]},\n","{\"df\": df_capacities , \"name\" : \"capacities\", \"keys\" : [\"capacity_id\"]},\n","{\"df\": df_lakehouses , \"name\" : \"lakehouses\", \"keys\" : [\"lakehouse_id\", \"workspace_id\"]},\n","{\"df\": df_lakehouse_users , \"name\" : \"lakehouses_users\", \"keys\" : [\"lakehouse_id\"]},\n","{\"df\": df_dashboards , \"name\" : \"dashboards\", \"keys\" : [\"dashboard_id\", \"workspace_id\"]},\n","{\"df\": df_dashboard_users , \"name\" : \"dashboard_users\", \"keys\" : [\"dashboard_id\"]},\n","{\"df\": df_warehouses , \"name\" : \"warehouses\",  \"keys\" : [\"warehouse_id\", \"workspace_id\"]},\n","{\"df\": df_warehouse_users , \"name\" : \"warehouse_users\", \"keys\" : [\"warehouse_id\"]},\n","{\"df\": df_eventstreams , \"name\" : \"eventstreams\",  \"keys\" : [\"eventstream_id\", \"workspace_id\"]},\n","{\"df\": df_eventstream_users, \"name\" : \"eventstream_users\",  \"keys\" : [\"eventstream_id\"]},\n","{\"df\": df_datapipelines , \"name\" : \"datapipelines\",  \"keys\" : [\"datapipeline_id\", \"workspace_id\"]},\n","{\"df\": df_datapipeline_users , \"name\" : \"datapipeline_users\",  \"keys\" : [\"datapipeline_id\"]},\n","{\"df\": df_notebooks , \"name\" : \"notebooks\",  \"keys\" : [\"notebook_id\", \"workspace_id\"]},\n","{\"df\": df_notebook_users , \"name\" : \"notebook_users\",  \"keys\" : [\"notebook_id\"]}\n","]\n","\n","for it in write_list:\n","    print(it[\"name\"])\n","    write_data(it[\"df\"], it[\"name\"], it[\"keys\"])"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"18e21189-c0dd-4272-9344-0b3eaf8d9ed0"},{"cell_type":"markdown","source":["Add date table with all existing dates in the data model. To be used by Power BI via Direct Lake"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"031cc752-ed8b-4b15-8f76-5d21e6c6b430"},{"cell_type":"code","source":["df_dates = spark.sql(\"\"\"\n","WITH all_dates as (\n","SELECT DISTINCT date FROM capacities\n",")\n","SELECT date, CASE WHEN date = (SELECT MAX(date) dat FROM all_dates) THEN true else false END AS Latest FROM all_dates\n","\"\"\")\n","df_dates.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").format(\"delta\").save(\"Tables/date\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{},"collapsed":false},"id":"3c8aa080-e7cf-43f3-ab24-216c398c0dea"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default"},"trident":{"lakehouse":{"default_lakehouse":"","default_lakehouse_name":"","default_lakehouse_workspace_id":""},"environment":{}}},"nbformat":4,"nbformat_minor":5}
